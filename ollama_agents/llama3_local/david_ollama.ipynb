{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    %pip install langchain_community\n",
    "    %pip install langchain_core\n",
    "    %pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this just tests whether our connection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"LLMs: The Game-Changers for Shipping Magnates Like Me\"**\n",
      "\n",
      "As a shipping magnate, I've spent my fair share of time navigating the complexities of global trade and commerce. From managing fleets of vessels to negotiating with ports and customs officials, it's a never-ending battle to stay ahead of the curve. But in recent years, I've noticed a new player on the field that has the potential to revolutionize our industry: Large Language Models (LLMs).\n",
      "\n",
      "At first, I was skeptical about these AI-powered language processing machines. What could they possibly do for me? But as I delved deeper into their capabilities, I realized that LLMs have the power to transform the way we operate in shipping.\n",
      "\n",
      "First and foremost, LLMs can help us streamline our communication processes. Gone are the days of tedious email exchanges and phone calls with customs officials, ports, and other stakeholders. With an LLM-powered chatbot, we can automate these interactions, reducing errors and increasing efficiency by a significant margin. No more misinterpreted instructions or lost documents – it's all about clarity and precision.\n",
      "\n",
      "But that's just the tip of the iceberg. LLMs can also help us optimize our logistics operations. By analyzing vast amounts of data on shipping patterns, traffic congestion, and weather conditions, these AI models can predict with uncanny accuracy when and where delays are likely to occur. This allows us to proactively adjust our routes, schedules, and cargo allocations, minimizing the risk of costly disruptions.\n",
      "\n",
      "And let's not forget about the power of predictive analytics. LLMs can analyze historical data on market trends, demand patterns, and supply chain dynamics to forecast future developments with remarkable accuracy. This enables us to make informed decisions about capacity planning, route optimization, and inventory management – all crucial components in our quest for operational excellence.\n",
      "\n",
      "But what really gets my attention is the potential for LLMs to revolutionize our customer service experience. Imagine being able to provide personalized support to our clients through AI-powered chatbots that understand their specific needs and preferences. No more tedious phone calls or email exchanges; just seamless, intuitive interactions that leave a lasting impression on our customers.\n",
      "\n",
      "Of course, there are also the obvious benefits of cost savings and increased productivity. By automating routine tasks and freeing up human resources for higher-value activities, we can reduce our operational expenses and focus on what really matters – growing our business and delivering exceptional service to our clients.\n",
      "\n",
      "As I look to the future, I'm excited about the possibilities that LLMs bring to the shipping industry. It's not just about technology for its own sake; it's about using AI to augment human capabilities, drive innovation, and create new opportunities for growth and success.\n",
      "\n",
      "In conclusion, Large Language Models are more than just a passing fad – they're game-changers for shipping magnates like me. By embracing these powerful tools, we can transform our operations, improve our customer service, and stay ahead of the curve in an increasingly competitive market. So, if you'll excuse me, I have some LLMs to deploy...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Local Llama3 \n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    keep_alive=-1, # keep the model loaded indefinitely\n",
    "    temperature=0,\n",
    "    max_new_tokens=512)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Write me a 500 word article on {topic} from the perspective of a {profession}. \")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LLMs\", \"profession\": \"shipping magnate\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And this shows the same as above - but using streaming instead of waiting for completed inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The life of a mushroom on a pizza is a good one. I've had my fair share of adventures, and let me tell you, it's been a wild ride. From the damp earth to the scorching hot oven, I've seen it all.\n",
      "\n",
      "It starts with the humble beginnings, growing in the dark, damp soil. It's a slow process, but eventually, I emerge as a tiny little thing, ready to take on the world. Or at least, that's what I thought. Little did I know, my fate was sealed the moment I was plucked from the earth and placed onto a pizza.\n",
      "\n",
      "The first thing I notice is the aroma. It's intoxicating, a mix of melted cheese, savory sauce, and spices that make my little mushroom head spin. I'm not sure what it is about the smell of pizza, but it's like nothing else in the world. And then there are the toppings - the gooey mozzarella, the crispy pepperoni, the tangy olives... it's a sensory overload.\n",
      "\n",
      "But as much as I love the aroma and the toppings, the real excitement comes when the pizza is placed into the oven. That's when things get hot, literally. The heat is intense, and I can feel myself cooking, my flavors melding together with the others on the pizza. It's like a little mushroom party in there.\n",
      "\n",
      "And then, just as quickly as it started, it's all over. The pizza is pulled out of the oven, and I'm left to cool off, surrounded by the remnants of what was once a delicious meal. It's bittersweet, really. On one hand, I'm proud of the role I played in bringing joy to whoever ate that pizza. On the other hand, it's hard not to feel a little sad when my time is up.\n",
      "\n",
      "But even in death, there's still life. The leftovers are often used to make something new - a salad, a sandwich, or maybe even another pizza. And who knows, maybe I'll get to be a part of that too. After all, as a mushroom, I'm all about recycling and reusing. It's the circle of life, right?\n",
      "\n",
      "As I look back on my time on that pizza, I realize that it wasn't just about me. It was about being a small but important part of something bigger than myself. And who knows, maybe someday I'll get to be a part of an even bigger something - like a pizza party or a food truck festival.\n",
      "\n",
      "But for now, I'm content with my role as a humble little mushroom on a pizza. It may not be the most glamorous job in the world, but it's one that brings joy and satisfaction to those who eat me. And what more could a mushroom ask for?"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"topic\": \"pizza\", \"profession\": \"mushroom\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's get some JSON back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"John\",\n",
      "    \"age\": 35,\n",
      "    \"hobbies\": [\n",
      "        \"pizza\"\n",
      "    ]\n",
      "}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "json_schema = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\n",
    "            \"title\": \"Name\",\n",
    "            \"description\": \"The person's name\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"age\": {\n",
    "            \"title\": \"Age\",\n",
    "            \"description\": \"The person's age\",\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 0\n",
    "        },\n",
    "        \"favorite_food\": {\n",
    "            \"title\": \"Favorite Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\", \"favorite_food\"]\n",
    "}\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    format=\"json\",\n",
    "    keep_alive=-1, # keep the model loaded indefinitely\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512\n",
    "    )\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Please tell me about a person using the following JSON schema:\"\n",
    "    ),\n",
    "    HumanMessage(content=\"{schema}\"),\n",
    "    HumanMessage(\n",
    "        content=\"Now, considering the schema, tell me about a person named John who is 35 years old and loves pizza.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "#converting the json schema to a string\n",
    "dumps = json.dumps(json_schema, indent=2)\n",
    "\n",
    "\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "response = chain.invoke({\"schema\": dumps})\n",
    "print(json.dumps(response, indent=4))\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you don't use JsonOutputParser we get a string instead of a dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"John\",\n",
      "    \"age\": 35,\n",
      "    \"hobbies\": [\n",
      "        \"pizza\"\n",
      "    ]\n",
      "}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "print(json.dumps(response, indent=4))\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at using the actul function calling / structured responses support in ollama \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Claudia' height=6.0 hair_color='brunette'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# Pydantic Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "context = \"\"\"Alex is 5 feet tall. \n",
    "Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "# Prompt template llama3\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a smart assistant take the following context and question below and return your answer in JSON.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "QUESTION: {question} \\n\n",
    "CONTEXT: {context} \\n\n",
    "JSON:\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    " \"\"\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm = OllamaFunctions(model=\"llama3\", \n",
    "                      format=\"json\", \n",
    "                      temperature=0)\n",
    "\n",
    "structured_llm = llm.with_structured_output(Person)\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"question\": \"Who is taller?\",\n",
    "    \"context\": context\n",
    "    })\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now it's the same thing but phi3 - only real diff is the prompt template\n",
    "\n",
    "however - it may output a pydantic error by not stickign to the tempalte - if that happens we need to try again with a variation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt: 0\n",
      "Attempt 1: Failed to parse with temperature 0. Trying again...\n",
      "attempt: 1\n",
      "Attempt 2: Failed to parse with temperature 0.1. Trying again...\n",
      "attempt: 2\n",
      "this took 2 attempts\n",
      "name='Claudia' height=6.0 hair_color='brunette'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "context = \"\"\"Alex is 5 feet tall. \n",
    "Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "\n",
    "# Prompt template phi 3\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"<|user|>{context}\n",
    "\n",
    "QUESTION: {question}<|end|>\n",
    "<|assistant|>AI: \"\"\"\n",
    ")\n",
    "\n",
    "# Function to adjust temperature and retry\n",
    "def invoke_with_adjusted_temperature(context, question, initial_temperature, max_attempts=10):\n",
    "    temperature = initial_temperature\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            print (f\"attempt: {attempts}\")\n",
    "            llm = OllamaFunctions(model=\"phi3\", format=\"json\", temperature=temperature)\n",
    "            structured_llm = llm.with_structured_output(Person)\n",
    "            chain = prompt | structured_llm\n",
    "            \n",
    "            response = chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"context\": context\n",
    "            })\n",
    "            print (f\"this took {attempts} attempts\")\n",
    "            return response  # Successful parsing\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempts + 1}: Failed to parse with temperature {temperature}. Trying again...\")\n",
    "            attempts += 1\n",
    "            temperature += 0.1  # Increment temperature\n",
    "            \n",
    "            if attempts == max_attempts:\n",
    "                print(\"Maximum attempts reached. Raising the last error encountered.\")\n",
    "                raise e  # Raise the last error after max attempts\n",
    "\n",
    "# Call the function\n",
    "try:\n",
    "    response = invoke_with_adjusted_temperature(context, \"Who is taller?\", initial_temperature=0)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(\"Final error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a random guy said the prompt format was wrong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt: 0\n",
      "this took 0 attempts\n",
      "name='Claudia' height=6.0 hair_color='brunette'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "context = \"\"\"Alex is 5 feet tall. \n",
    "Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "\n",
    "# Prompt template phi 3 - from some guy\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"{context}\n",
    "    Human: {question}\n",
    "    AI:\"\"\"\n",
    ")\n",
    "\n",
    "# Function to adjust temperature and retry\n",
    "def invoke_with_adjusted_temperature(context, question, initial_temperature, max_attempts=10):\n",
    "    temperature = initial_temperature\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            print (f\"attempt: {attempts}\")\n",
    "            llm = OllamaFunctions(model=\"phi3\", format=\"json\", temperature=temperature)\n",
    "            structured_llm = llm.with_structured_output(Person)\n",
    "            chain = prompt | structured_llm\n",
    "            \n",
    "            response = chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"context\": context\n",
    "            })\n",
    "            print (f\"this took {attempts} attempts\")\n",
    "            return response  # Successful parsing\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempts + 1}: Failed to parse with temperature {temperature}. Trying again...\")\n",
    "            attempts += 1\n",
    "            temperature += 0.1  # Increment temperature\n",
    "            \n",
    "            if attempts == max_attempts:\n",
    "                print(\"Maximum attempts reached. Raising the last error encountered.\")\n",
    "                raise e  # Raise the last error after max attempts\n",
    "\n",
    "# Call the function\n",
    "try:\n",
    "    response = invoke_with_adjusted_temperature(context, \"Who is taller?\", initial_temperature=0)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(\"Final error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Singapore\", \"unit\": \"celsius\"}'}} id='run-c366024f-0e37-460a-a059-96c1b85b69df-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = OllamaFunctions(\n",
    "    model=\"llama3\", \n",
    "    format=\"json\"\n",
    "    )\n",
    "\n",
    "model = model.bind_tools(\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "\n",
    "response = model.invoke(\"what is the weather in Singapore?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and same in phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Singapore\"}'}} id='run-431d538b-40cd-4b63-b69c-702345a9f273-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = OllamaFunctions(\n",
    "    model=\"phi3\", \n",
    "    keep_alive=-1,\n",
    "    format=\"json\"\n",
    "    )\n",
    "\n",
    "model = model.bind_tools(\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "\n",
    "response = model.invoke(\"what is the weather in Singapore?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
