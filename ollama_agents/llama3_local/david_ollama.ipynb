{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_experimental\n",
      "  Downloading langchain_experimental-0.0.58-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain<0.2.0,>=0.1.17 (from langchain_experimental)\n",
      "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain_experimental) (0.1.52)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/david/.local/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (3.8.6)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/david/.local/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (0.0.38)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.17->langchain_experimental)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (0.1.57)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/david/.local/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (1.26.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/david/.local/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (8.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain_experimental) (23.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/david/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/david/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/david/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/david/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/david/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain_experimental) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.17->langchain_experimental) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/david/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.17->langchain_experimental) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/david/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.17->langchain_experimental) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.17->langchain_experimental) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.17->langchain_experimental) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.17->langchain_experimental) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.17->langchain_experimental) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.17->langchain_experimental) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/david/anaconda3/envs/p310/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental) (1.0.0)\n",
      "Downloading langchain_experimental-0.0.58-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: langchain-text-splitters, langchain, langchain_experimental\n",
      "Successfully installed langchain-0.1.20 langchain-text-splitters-0.0.1 langchain_experimental-0.0.58\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    %pip install langchain_community\n",
    "    %pip install langchain_core\n",
    "    %pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this just tests whether our connection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"LLMs: The Game-Changers for Shipping Magnates Like Me\"**\n",
      "\n",
      "As a shipping magnate, I've spent my fair share of time navigating the complexities of global trade and commerce. From managing fleets of vessels to negotiating with ports and customs officials, there's no shortage of challenges in this industry. But one development that has caught my attention - and potentially changed the game for me and my competitors alike - is the rise of Large Language Models (LLMs).\n",
      "\n",
      "At first glance, LLMs might seem like a distant cousin to the world of shipping. After all, they're AI-powered language processing systems designed to analyze and generate human-like text. But trust me, their impact on our industry has been nothing short of profound.\n",
      "\n",
      "Let's start with the obvious: data analysis. As a shipping magnate, I need to stay on top of market trends, demand patterns, and supply chain disruptions. LLMs have revolutionized my ability to do just that. By processing vast amounts of text-based data - from news articles to social media posts to industry reports - these AI models can identify key insights and patterns that would take human analysts weeks or even months to uncover.\n",
      "\n",
      "For instance, I recently used an LLM to analyze a dataset of global trade news stories. In mere minutes, the model highlighted emerging trends in container shipping, such as increased demand for Asian-to-European routes and growing concerns about supply chain resilience. This information allowed me to adjust my fleet's deployment strategy, optimizing our operations to capitalize on these shifting market dynamics.\n",
      "\n",
      "Another area where LLMs have made a significant difference is in customer service. As the world becomes increasingly digital, customers expect seamless interactions with companies - including shipping lines like mine. By integrating an LLM into our customer support system, we've been able to provide faster, more accurate responses to queries and concerns. This has not only improved customer satisfaction but also reduced the workload for our human customer service reps.\n",
      "\n",
      "But perhaps the most exciting application of LLMs in shipping is in predictive analytics. Imagine being able to forecast with high accuracy when a particular route will experience congestion or when a specific commodity's demand will surge. That's exactly what these AI models can do, by analyzing historical data and identifying patterns that might not be immediately apparent to human analysts.\n",
      "\n",
      "For example, I used an LLM to predict the likelihood of a certain container ship route experiencing delays due to weather-related issues. The model analyzed historical data on storms in the region, combined with real-time weather forecasts, to provide me with a probability score indicating the likelihood of delays. This allowed me to proactively adjust our scheduling and logistics to minimize disruptions.\n",
      "\n",
      "Of course, there are also concerns about the potential impact of LLMs on human jobs in the shipping industry. As AI takes over more routine tasks, some worry that it will displace workers. While this is a valid concern, I believe that LLMs will ultimately augment human capabilities rather than replace them. By freeing up our employees to focus on higher-value tasks like strategy and decision-making, we can actually improve overall efficiency and productivity.\n",
      "\n",
      "In conclusion, Large Language Models have the potential to revolutionize the shipping industry in ways both big and small. From data analysis to customer service to predictive analytics, these AI-powered tools are already making a significant impact on my business - and I'm confident they will continue to do so as they evolve and improve. As a shipping magnate, I'm excited to see how LLMs will shape the future of our industry, and I'm committed to staying at the forefront of this technological revolution.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Local Llama3 \n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    keep_alive=-1, # keep the model loaded indefinitely\n",
    "    temperature=0,\n",
    "    max_new_tokens=512)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Write me a 500 word article on {topic} from the perspective of a {profession}. \")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LLMs\", \"profession\": \"shipping magnate\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And this shows the same as above - but using streaming instead of waiting for completed inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The life of a mushroom on a pizza is a good one. I've had my fair share of adventures, and let me tell you, it's been a wild ride. From the damp earth to the scorching hot oven, I've seen it all.\n",
      "\n",
      "It starts with the humble beginnings, growing in the dark, damp soil. It's a slow process, but eventually, I emerge as a tiny little thing, ready to take on the world. Or at least, that's what I thought. Little did I know, my fate was sealed the moment I was plucked from the earth and placed onto a pizza.\n",
      "\n",
      "The first thing I notice is the aroma. It's intoxicating, a mix of melted cheese, savory sauce, and spices that make my little mushroom head spin. I'm not sure what it is about the smell of pizza, but it's like nothing else in the world. And then there are the toppings - the gooey mozzarella, the crispy pepperoni, the tangy olives... it's a sensory overload.\n",
      "\n",
      "But as much as I love the aroma and the toppings, the real excitement comes when the pizza is placed into the oven. That's when things get hot, literally. The heat is intense, and I can feel myself cooking, my flavors melding together with the others on the pizza. It's like a little mushroom party in there.\n",
      "\n",
      "And then, just as quickly as it started, it's all over. The pizza is pulled out of the oven, and I'm left to cool off, surrounded by the remnants of what was once a delicious meal. It's bittersweet, really. On one hand, I'm proud of the role I played in bringing joy to whoever ate that pizza. On the other hand, it's hard not to feel a little sad when my time is up.\n",
      "\n",
      "But even in death, there's still life. The leftovers are often used to make something new - a salad, a sandwich, or maybe even another pizza. And who knows, maybe I'll get to be a part of that too. After all, as a mushroom, I'm all about recycling and reusing. It's the circle of life, right?\n",
      "\n",
      "As I look back on my time on that pizza, I realize that it wasn't just about me. It was about being a small but important part of something bigger than myself. And who knows, maybe someday I'll get to be a part of an even bigger something - like a pizza party or a food truck festival.\n",
      "\n",
      "But for now, I'm content with my role as a humble little mushroom on a pizza. It may not be the most glamorous job in the world, but it's one that brings joy and satisfaction to those who eat me. And what more could a mushroom ask for?"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"topic\": \"pizza\", \"profession\": \"mushroom\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's get some JSON back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{\\\"name\\\": \\\"John\\\", \\\"age\\\": 35, \\\"hobbies\\\": [\\\"pizza\\\"]}\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \"\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "json_schema = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\n",
    "            \"title\": \"Name\",\n",
    "            \"description\": \"The person's name\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"age\": {\n",
    "            \"title\": \"Age\",\n",
    "            \"description\": \"The person's age\",\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 0\n",
    "        },\n",
    "        \"favorite_food\": {\n",
    "            \"title\": \"Favorite Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\", \"favorite_food\"]\n",
    "}\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    format=\"json\",\n",
    "    keep_alive=-1, # keep the model loaded indefinitely\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512\n",
    "    )\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Please tell me about a person using the following JSON schema:\"\n",
    "    ),\n",
    "    HumanMessage(content=\"{schema}\"),\n",
    "    HumanMessage(\n",
    "        content=\"Now, considering the schema, tell me about a person named John who is 35 years old and loves pizza.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "#converting the json schema to a string\n",
    "dumps = json.dumps(json_schema, indent=2)\n",
    "\n",
    "\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "response = chain.invoke({\"schema\": dumps})\n",
    "print(json.dumps(response, indent=4))\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you don't use JsonOutputParser we get a string instead of a dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{\\\"name\\\": \\\"John\\\", \\\"age\\\": 35, \\\"hobbies\\\": [\\\"pizza\\\"]}\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \"\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "print(json.dumps(response, indent=4))\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at using the actul function calling / structured responses support in ollama \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Claudia' height=6.0 hair_color='brunette'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# Pydantic Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "context = \"\"\"Alex is 5 feet tall. \n",
    "Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "# Prompt template llama3\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a smart assistant take the following context and question below and return your answer in JSON.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "QUESTION: {question} \\n\n",
    "CONTEXT: {context} \\n\n",
    "JSON:\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    " \"\"\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm = OllamaFunctions(model=\"llama3\", \n",
    "                      format=\"json\", \n",
    "                      temperature=0)\n",
    "\n",
    "structured_llm = llm.with_structured_output(Person)\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"question\": \"Who is taller?\",\n",
    "    \"context\": context\n",
    "    })\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now it's the same thing but phi3 - only real diff is the prompt template\n",
    "\n",
    "however - it may output a pydantic error by not stickign to the tempalte - if that happens we need to try again with a variation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt: 0\n",
      "Attempt 1: Failed to parse with temperature 0. Trying again...\n",
      "attempt: 1\n",
      "Attempt 2: Failed to parse with temperature 0.1. Trying again...\n",
      "attempt: 2\n",
      "Attempt 3: Failed to parse with temperature 0.2. Trying again...\n",
      "attempt: 3\n",
      "Attempt 4: Failed to parse with temperature 0.30000000000000004. Trying again...\n",
      "attempt: 4\n",
      "Attempt 5: Failed to parse with temperature 0.4. Trying again...\n",
      "attempt: 5\n",
      "Attempt 6: Failed to parse with temperature 0.5. Trying again...\n",
      "attempt: 6\n",
      "this took 6 attempts\n",
      "name='Claudia' height=6.0 hair_color='brunette'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "context = \"\"\"Alex is 5 feet tall. \n",
    "Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "\n",
    "# Prompt template phi 3\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"<|user|>{context}\n",
    "\n",
    "QUESTION: {question}<|end|>\n",
    "<|assistant|>AI: \"\"\"\n",
    ")\n",
    "\n",
    "# Function to adjust temperature and retry\n",
    "def invoke_with_adjusted_temperature(context, question, initial_temperature, max_attempts=10):\n",
    "    temperature = initial_temperature\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            print (f\"attempt: {attempts}\")\n",
    "            llm = OllamaFunctions(model=\"phi3\", format=\"json\", temperature=temperature)\n",
    "            structured_llm = llm.with_structured_output(Person)\n",
    "            chain = prompt | structured_llm\n",
    "            \n",
    "            response = chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"context\": context\n",
    "            })\n",
    "            print (f\"this took {attempts} attempts\")\n",
    "            return response  # Successful parsing\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempts + 1}: Failed to parse with temperature {temperature}. Trying again...\")\n",
    "            attempts += 1\n",
    "            temperature += 0.1  # Increment temperature\n",
    "            \n",
    "            if attempts == max_attempts:\n",
    "                print(\"Maximum attempts reached. Raising the last error encountered.\")\n",
    "                raise e  # Raise the last error after max attempts\n",
    "\n",
    "# Call the function\n",
    "try:\n",
    "    response = invoke_with_adjusted_temperature(context, \"Who is taller?\", initial_temperature=0)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(\"Final error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a random guy said the prompt format was wrong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt: 0\n",
      "this took 0 attempts\n",
      "name='Claudia' height=6.0 hair_color='brunette'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "context = \"\"\"Alex is 5 feet tall. \n",
    "Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "\n",
    "# Prompt template phi 3 - from some guy\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"{context}\n",
    "    Human: {question}\n",
    "    AI:\"\"\"\n",
    ")\n",
    "\n",
    "# Function to adjust temperature and retry\n",
    "def invoke_with_adjusted_temperature(context, question, initial_temperature, max_attempts=10):\n",
    "    temperature = initial_temperature\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            print (f\"attempt: {attempts}\")\n",
    "            llm = OllamaFunctions(model=\"phi3\", format=\"json\", temperature=temperature)\n",
    "            structured_llm = llm.with_structured_output(Person)\n",
    "            chain = prompt | structured_llm\n",
    "            \n",
    "            response = chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"context\": context\n",
    "            })\n",
    "            print (f\"this took {attempts} attempts\")\n",
    "            return response  # Successful parsing\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempts + 1}: Failed to parse with temperature {temperature}. Trying again...\")\n",
    "            attempts += 1\n",
    "            temperature += 0.1  # Increment temperature\n",
    "            \n",
    "            if attempts == max_attempts:\n",
    "                print(\"Maximum attempts reached. Raising the last error encountered.\")\n",
    "                raise e  # Raise the last error after max attempts\n",
    "\n",
    "# Call the function\n",
    "try:\n",
    "    response = invoke_with_adjusted_temperature(context, \"Who is taller?\", initial_temperature=0)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(\"Final error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Singapore\", \"unit\": \"celsius\"}'}} id='run-71389e7d-2d14-4107-9aa0-ed84d064a7c2-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = OllamaFunctions(\n",
    "    model=\"llama3\", \n",
    "    format=\"json\"\n",
    "    )\n",
    "\n",
    "model = model.bind_tools(\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "\n",
    "response = model.invoke(\"what is the weather in Singapore?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and same in phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Singapore\"}'}} id='run-ad611cdb-de60-43e2-a00e-b28b6fc4dea0-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = OllamaFunctions(\n",
    "    model=\"phi3\", \n",
    "    keep_alive=-1,\n",
    "    format=\"json\"\n",
    "    )\n",
    "\n",
    "model = model.bind_tools(\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "\n",
    "response = model.invoke(\"what is the weather in Singapore?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
